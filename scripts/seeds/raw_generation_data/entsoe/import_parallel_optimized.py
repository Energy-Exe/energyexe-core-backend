"""Ultra-optimized parallel import of ENTSOE Excel files with all performance improvements."""

import asyncio
import sys
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np
from tqdm import tqdm
import os
from multiprocessing import Process, Queue, current_process, cpu_count
from typing import Dict, Any, List, Set, Optional
import signal
import time
import pickle
import psutil
from io import StringIO
import tempfile
import json

# Try to import optional faster libraries
try:
    import openpyxl
    HAS_OPENPYXL = True
except ImportError:
    HAS_OPENPYXL = False
    print("‚ö†Ô∏è openpyxl not installed. Using default Excel reader (slower)")

# Add parent directories to path
current_dir = Path(__file__).parent
sys.path.append(str(current_dir.parent.parent.parent.parent))

from app.core.database import get_session_factory
from app.core.config import get_settings
from app.models.generation_data import GenerationDataRaw
from app.models.generation_unit import GenerationUnit
from sqlalchemy import select, func, text
from sqlalchemy.ext.asyncio import AsyncSession
import asyncpg


def signal_handler(signum, frame):
    """Handle interrupt signal gracefully."""
    print(f"\n‚ö†Ô∏è Process {current_process().name} interrupted")
    sys.exit(0)


def get_optimal_chunk_size(file_size_mb: float) -> int:
    """Dynamically determine optimal chunk size based on available memory."""
    
    available_memory = psutil.virtual_memory().available
    
    # Use 10% of available memory for chunks
    memory_for_chunks = available_memory * 0.1
    
    # Estimate row size for ENTSOE data (about 500 bytes per row with all columns)
    estimated_row_size = 500
    optimal_chunk = int(memory_for_chunks / estimated_row_size)
    
    # Clamp between reasonable bounds
    min_chunk = 5000
    max_chunk = 50000  # Smaller than ELEXON due to Excel overhead
    
    chunk_size = max(min_chunk, min(optimal_chunk, max_chunk))
    
    print(f"[Optimization] Using chunk size: {chunk_size:,} (Available memory: {available_memory / 1e9:.1f}GB)")
    return chunk_size


async def get_relevant_unit_codes() -> Set[str]:
    """Fetch generation unit codes from database where source='ENTSOE'."""
    
    AsyncSessionLocal = get_session_factory()
    
    async with AsyncSessionLocal() as db:
        result = await db.execute(
            select(GenerationUnit.code)
            .where(GenerationUnit.source == 'ENTSOE')
        )
        unit_codes = {row[0] for row in result}
    
    print(f"Found {len(unit_codes)} relevant ENTSOE generation units in the system")
    return unit_codes


async def import_with_copy(
    db_url: str,
    filtered_df: pd.DataFrame,
    table_name: str = 'generation_data_raw'
) -> int:
    """Use PostgreSQL COPY for ultra-fast bulk insert."""
    
    if filtered_df.empty:
        return 0
    
    # Prepare data for COPY
    output = StringIO()
    
    # Parse DateTime column and handle resolution
    filtered_df['DateTime (UTC)'] = pd.to_datetime(filtered_df['DateTime (UTC)'])
    
    # Calculate period_end based on resolution
    def calculate_period_end(row):
        if row['ResolutionCode'] == 'PT60M':
            return row['DateTime (UTC)'] + pd.Timedelta(hours=1)
        elif row['ResolutionCode'] == 'PT15M':
            return row['DateTime (UTC)'] + pd.Timedelta(minutes=15)
        else:
            return row['DateTime (UTC)'] + pd.Timedelta(hours=1)  # Default to hourly
    
    filtered_df['period_start'] = filtered_df['DateTime (UTC)']
    filtered_df['period_end'] = filtered_df.apply(calculate_period_end, axis=1)
    
    # Make timezone-aware (ENTSOE data is in UTC)
    filtered_df['period_start'] = filtered_df['period_start'].dt.tz_localize('UTC')
    filtered_df['period_end'] = filtered_df['period_end'].dt.tz_localize('UTC')
    
    # Columns that exist in generation_data_raw table
    columns = ['source', 'source_type', 'identifier', 'period_type', 'period_start', 'period_end', 
               'value_extracted', 'unit', 'data']
    
    # Add required columns with correct values
    filtered_df['source'] = 'ENTSOE'
    filtered_df['source_type'] = 'excel'  # Since we're importing from Excel files
    filtered_df['identifier'] = filtered_df['GenerationUnitCode']
    filtered_df['period_type'] = filtered_df['ResolutionCode']
    filtered_df['value_extracted'] = filtered_df['ActualGenerationOutput(MW)']
    filtered_df['unit'] = 'MW'
    
    # Create JSONB data column with all original data
    filtered_df['data'] = filtered_df.apply(
        lambda row: json.dumps({
            'area_code': row.get('AreaCode', ''),
            'area_display_name': row.get('AreaDisplayName', ''),
            'area_type_code': row.get('AreaTypeCode', ''),
            'map_code': row.get('MapCode', ''),
            'generation_unit_code': row.get('GenerationUnitCode', ''),
            'generation_unit_name': row.get('GenerationUnitName', ''),
            'generation_unit_type': row.get('GenerationUnitType', ''),
            'actual_generation_output_mw': float(row.get('ActualGenerationOutput(MW)', 0)) if pd.notna(row.get('ActualGenerationOutput(MW)')) else None,
            'actual_consumption_mw': float(row.get('ActualConsumption(MW)', 0)) if pd.notna(row.get('ActualConsumption(MW)')) else None,
            'installed_capacity_mw': int(row.get('GenerationUnitInstalledCapacity(MW)', 0)) if pd.notna(row.get('GenerationUnitInstalledCapacity(MW)')) else None,
            'resolution_code': row.get('ResolutionCode', ''),
            'update_time': str(row.get('UpdateTime(UTC)', ''))
        }), axis=1
    )
    
    # Connect directly with asyncpg for COPY
    try:
        conn = await asyncpg.connect(db_url)
        
        # Convert DataFrame to list of tuples for copy_records_to_table
        records = []
        for _, row in filtered_df.iterrows():
            # Ensure proper data types for each column
            record = (
                row['source'],
                row['source_type'],
                row['identifier'],
                row['period_type'],
                row['period_start'],  # Already datetime
                row['period_end'],    # Already datetime
                float(row['value_extracted']) if pd.notna(row['value_extracted']) else None,
                row['unit'],
                row['data']  # JSON string
            )
            records.append(record)
        
        # Use copy_records_to_table which is the correct asyncpg method
        result = await conn.copy_records_to_table(
            table_name,
            records=records,
            columns=columns
        )
        
        await conn.close()
        return len(records)
        
    except Exception as e:
        print(f"COPY failed: {e}, using fallback bulk insert...")
        
        # Fallback: Use SQLAlchemy bulk insert
        try:
            from app.core.database import get_session_factory
            from app.models.generation_data import GenerationDataRaw
            
            # Convert DataFrame to list of dicts for bulk insert
            records_dict = filtered_df[columns].to_dict('records')
            
            # Use sync operation in async context
            async def bulk_insert():
                AsyncSessionLocal = get_session_factory()
                async with AsyncSessionLocal() as db:
                    # Convert to GenerationDataRaw objects
                    objects = [
                        GenerationDataRaw(**record)
                        for record in records_dict
                    ]
                    db.add_all(objects)
                    await db.commit()
                    return len(objects)
            
            return await bulk_insert()
            
        except Exception as e2:
            print(f"Bulk insert also failed: {e2}")
            return 0


def convert_excel_to_csv_optimized(excel_path: str, csv_path: str) -> int:
    """Convert Excel to CSV for faster processing."""
    
    # Read Excel with optimizations
    if HAS_OPENPYXL:
        df = pd.read_excel(excel_path, engine='openpyxl')
    else:
        df = pd.read_excel(excel_path)
    
    # Save as CSV for faster reading later
    df.to_csv(csv_path, index=False)
    
    return len(df)


def import_single_file_worker_optimized(
    file_path: str,
    worker_id: int,
    relevant_unit_codes: Set[str],
    db_url: str,
    skip_duplicates: bool = True,
    use_copy: bool = True,
    batch_size: int = 5,
    progress_queue: Queue = None,
    status_queue: Queue = None
) -> Dict[str, Any]:
    """Optimized worker that processes an Excel file with filtering."""
    
    # Set up signal handler
    signal.signal(signal.SIGINT, signal_handler)
    
    file_name = os.path.basename(file_path)
    worker_name = f"Worker-{worker_id}"
    
    # Send status update
    if status_queue:
        status_queue.put({
            'worker': worker_id,
            'status': 'starting',
            'file': file_name
        })
    
    print(f"\n[{worker_name}] üöÄ Starting: {file_name}")
    
    # Get file size for optimization
    file_size_mb = os.path.getsize(file_path) / 1024 / 1024
    print(f"[{worker_name}] üìÅ File size: {file_size_mb:.2f} MB")
    
    # Convert Excel to CSV for faster processing
    temp_csv = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)
    temp_csv_path = temp_csv.name
    temp_csv.close()
    
    try:
        print(f"[{worker_name}] üìÑ Converting Excel to CSV for faster processing...")
        total_rows = convert_excel_to_csv_optimized(file_path, temp_csv_path)
        print(f"[{worker_name}] üìä Total rows: {total_rows:,}")
        
        if total_rows == 0:
            return {
                'file': file_name,
                'worker': worker_id,
                'total_rows': 0,
                'records_imported': 0,
                'records_filtered': 0
            }
        
        total_imported = 0
        total_filtered = 0
        start_time = datetime.now()
        
        # Get optimal chunk size
        chunk_size = get_optimal_chunk_size(file_size_mb)
        
        # Read CSV in chunks
        print(f"[{worker_name}] üìñ Reading CSV in chunks of {chunk_size:,} rows...")
        
        chunk_iterator = pd.read_csv(
            temp_csv_path,
            chunksize=chunk_size,
            dtype={
                'GenerationUnitCode': str,
                'GenerationUnitName': str,
                'GenerationUnitType': str,
                'ResolutionCode': str,
                'AreaCode': str,
                'AreaDisplayName': str,
                'MapCode': str,
                'ActualGenerationOutput(MW)': float,
                'ActualConsumption(MW)': float,
                'GenerationUnitInstalledCapacity(MW)': float
            }
        )
        
        pbar = tqdm(
            total=total_rows,
            desc=f"[W{worker_id}] {file_name[:30]}",
            unit="rows",
            position=worker_id,
            leave=True,
            colour='green',
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )
        
        # Accumulate batches for bulk processing
        accumulated_dfs = []
        
        for chunk_df in chunk_iterator:
            original_size = len(chunk_df)
            
            # Filter for relevant generation units
            filtered_df = chunk_df[chunk_df['GenerationUnitCode'].isin(relevant_unit_codes)]
            
            filtered_out = original_size - len(filtered_df)
            total_filtered += filtered_out
            
            pbar.update(original_size)
            
            if filtered_df.empty:
                continue
            
            # Accumulate for batch processing
            accumulated_dfs.append(filtered_df)
            
            # Process accumulated batches
            if len(accumulated_dfs) >= batch_size:
                # Combine batches
                combined_df = pd.concat(accumulated_dfs, ignore_index=True)
                
                # Import batch
                if use_copy and not combined_df.empty:
                    records = asyncio.run(import_with_copy(db_url, combined_df))
                    total_imported += records
                
                accumulated_dfs = []
                
                pbar.set_postfix({
                    '‚úÖ': f'{total_imported:,}',
                    'üö´': f'{total_filtered:,}',
                    'üìä': f'{(total_imported/(total_imported+total_filtered)*100):.1f}%' if (total_imported+total_filtered) > 0 else '0%'
                })
        
        # Process remaining accumulated data
        if accumulated_dfs:
            combined_df = pd.concat(accumulated_dfs, ignore_index=True)
            
            if use_copy and not combined_df.empty:
                records = asyncio.run(import_with_copy(db_url, combined_df))
                total_imported += records
        
        pbar.close()
        
    except Exception as e:
        print(f"\n[{worker_name}] Error: {str(e)}")
        return {
            'file': file_name,
            'worker': worker_id,
            'error': str(e)
        }
    finally:
        # Clean up temp file
        if os.path.exists(temp_csv_path):
            os.unlink(temp_csv_path)
    
    duration = (datetime.now() - start_time).total_seconds()
    filter_rate = (total_filtered / total_rows * 100) if total_rows > 0 else 0
    import_rate = total_imported/duration if duration > 0 else 0
    
    # Send completion status
    if status_queue:
        status_queue.put({
            'worker': worker_id,
            'status': 'completed',
            'imported': total_imported,
            'filtered': total_filtered,
            'duration': duration
        })
    
    print(f"\n[{worker_name}] üéâ Completed {file_name}:")
    print(f"  üìä Statistics:")
    print(f"     ‚Ä¢ Total rows processed: {total_rows:,}")
    print(f"     ‚Ä¢ Rows imported: {total_imported:,} ({100-filter_rate:.1f}%)")
    print(f"     ‚Ä¢ Rows filtered: {total_filtered:,} ({filter_rate:.1f}%)")
    print(f"  ‚è±Ô∏è  Performance:")
    print(f"     ‚Ä¢ Duration: {duration/60:.1f} minutes")
    print(f"     ‚Ä¢ Import rate: {import_rate:.0f} records/second")
    print(f"     ‚Ä¢ Processing speed: {total_rows/duration:.0f} rows/second" if duration > 0 else "")
    
    return {
        'file': file_name,
        'worker': worker_id,
        'total_rows': total_rows,
        'records_imported': total_imported,
        'records_filtered': total_filtered,
        'filter_rate': filter_rate,
        'duration_seconds': duration
    }


def run_optimized_worker(
    file_path: str,
    worker_id: int,
    unit_codes_file: str,
    db_url: str,
    skip_duplicates: bool,
    use_copy: bool,
    result_queue: Queue,
    progress_queue: Queue,
    status_queue: Queue = None
):
    """Run the optimized worker in a separate process."""
    try:
        # Load unit codes
        with open(unit_codes_file, 'rb') as f:
            relevant_unit_codes = pickle.load(f)
        
        result = import_single_file_worker_optimized(
            file_path,
            worker_id,
            relevant_unit_codes,
            db_url,
            skip_duplicates,
            use_copy,
            batch_size=5,
            progress_queue=progress_queue,
            status_queue=status_queue
        )
        result_queue.put(result)
        
    except KeyboardInterrupt:
        print(f"\nWorker {worker_id} interrupted")
        result_queue.put({
            'file': os.path.basename(file_path),
            'worker': worker_id,
            'error': 'Interrupted'
        })
    except Exception as e:
        print(f"\nWorker {worker_id} crashed: {e}")
        result_queue.put({
            'file': os.path.basename(file_path),
            'worker': worker_id,
            'error': str(e)
        })


async def get_database_stats() -> Dict[str, Any]:
    """Get current database statistics using fast estimation."""
    AsyncSessionLocal = get_session_factory()
    
    async with AsyncSessionLocal() as db:
        # Count ENTSOE records
        entsoe_result = await db.execute(
            select(func.count(GenerationDataRaw.id))
            .where(GenerationDataRaw.source == 'ENTSOE')
        )
        entsoe_count = entsoe_result.scalar() or 0
        
        # Use fast estimate for total
        stats_result = await db.execute(
            text("""
                SELECT 
                    reltuples::BIGINT as estimated_count,
                    pg_size_pretty(pg_total_relation_size('generation_data_raw')) as table_size
                FROM pg_class 
                WHERE relname = 'generation_data_raw'
            """)
        )
        stats = stats_result.first()
        
        return {
            'entsoe_count': entsoe_count,
            'total_count': stats.estimated_count if stats else 0,
            'table_size': stats.table_size if stats else 'Unknown'
        }


async def run_all_async_operations(
    skip_duplicates: bool = True,
    use_copy: bool = True
):
    """Run all async operations in a single event loop."""
    
    # Get relevant unit codes
    print("\nüîç Fetching relevant ENTSOE generation unit codes...")
    relevant_unit_codes = await get_relevant_unit_codes()
    
    if not relevant_unit_codes:
        print("\n‚ö†Ô∏è No generation units found with source='ENTSOE'")
        return None
    
    print(f"Will filter for {len(relevant_unit_codes)} generation unit codes")
    
    # Get initial stats
    print("\nüìä Checking database...")
    initial_stats = await get_database_stats()
    print(f"Current ENTSOE records: {initial_stats['entsoe_count']:,}")
    print(f"Total table size: {initial_stats['table_size']}")
    
    return {
        'relevant_unit_codes': relevant_unit_codes,
        'initial_stats': initial_stats
    }


def import_parallel_optimized(
    num_workers: int = 4,
    skip_duplicates: bool = True,
    use_copy: bool = True,
    max_files: Optional[int] = None
):
    """Ultra-optimized parallel import with all performance improvements."""
    
    print("\n" + "="*80)
    print(" "*20 + "üöÄ ENTSOE PARALLEL IMPORT üöÄ")
    print("="*80)
    
    # Show optimization status
    print("\nüìã Optimizations enabled:")
    print(f"  {'‚úÖ' if HAS_OPENPYXL else '‚ö†Ô∏è'} Openpyxl Excel reading: {'Yes' if HAS_OPENPYXL else 'No (install openpyxl for faster Excel reading)'}") 
    print(f"  ‚úÖ Excel to CSV conversion: Yes")
    print(f"  {'‚úÖ' if use_copy else '‚ùå'} PostgreSQL COPY: {'Yes' if use_copy else 'No'}")
    print(f"  {'‚úÖ' if not skip_duplicates else '‚ö†Ô∏è'} Memory duplicate check: {'No (faster)' if not skip_duplicates else 'Yes'}")
    print(f"  ‚úÖ Dynamic chunk sizing: Yes")
    print(f"  ‚úÖ Batch accumulation: Yes")
    print(f"  ‚úÖ Parallel processing: {num_workers} workers")
    
    # Run all async operations in a single event loop
    async_result = asyncio.run(run_all_async_operations(skip_duplicates, use_copy))
    
    if not async_result:
        return
    
    relevant_unit_codes = async_result['relevant_unit_codes']
    initial_stats = async_result['initial_stats']
    
    # Save unit codes for workers
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl')
    with open(temp_file.name, 'wb') as f:
        pickle.dump(relevant_unit_codes, f)
    unit_codes_file = temp_file.name
    
    # Get database URL
    settings = get_settings()
    # Get the base PostgreSQL URL for asyncpg (without the +asyncpg suffix)
    db_url = settings.database_url_async.replace('+asyncpg', '')
    
    # Get Excel files
    data_folder = Path(__file__).parent / "data"
    excel_files = sorted(data_folder.glob("*.xlsx"))
    
    if not excel_files:
        print(f"No Excel files found in {data_folder}")
        os.unlink(unit_codes_file)
        return
    
    if max_files:
        excel_files = excel_files[:max_files]
    
    print(f"\nüìÅ Found {len(excel_files)} Excel files")
    
    # Show file size summary
    total_size = sum(os.path.getsize(f) for f in excel_files)
    print(f"üìä Total data size: {total_size / 1024 / 1024:.2f} MB")
    
    # Confirm
    response = input("\nProceed with optimized import? (yes/no): ")
    if response.lower() != 'yes':
        print("Cancelled")
        os.unlink(unit_codes_file)
        return
    
    # Start workers
    actual_workers = min(num_workers, len(excel_files))
    print(f"\nüöÄ Starting {actual_workers} workers...")
    
    result_queue = Queue()
    progress_queue = Queue()
    status_queue = Queue()
    
    workers = []
    start_time = datetime.now()
    
    print("\n" + "="*80)
    print(" "*25 + "üìä IMPORT PROGRESS üìä")
    print("="*80)
    
    # Add spacing for progress bars
    for _ in range(actual_workers):
        print()
    
    # Start workers
    for i in range(actual_workers):
        if i < len(excel_files):
            p = Process(
                target=run_optimized_worker,
                args=(
                    str(excel_files[i]),
                    i,
                    unit_codes_file,
                    db_url,
                    skip_duplicates,
                    use_copy,
                    result_queue,
                    progress_queue,
                    status_queue
                ),
                name=f"Worker-{i}"
            )
            p.start()
            workers.append((p, i))
            time.sleep(0.2)
    
    print(f"\nüìà Processing with {actual_workers} workers...\n")
    
    # Monitor progress and assign new files to completed workers
    results = []
    next_file_index = actual_workers
    active_workers = dict(workers)
    
    try:
        while active_workers or next_file_index < len(excel_files):
            # Check for completed workers
            for p, worker_id in list(active_workers.items()):
                if not p.is_alive():
                    # Worker completed
                    if not result_queue.empty():
                        result = result_queue.get()
                        results.append(result)
                    
                    del active_workers[p]
                    
                    # Assign next file if available
                    if next_file_index < len(excel_files):
                        new_p = Process(
                            target=run_optimized_worker,
                            args=(
                                str(excel_files[next_file_index]),
                                worker_id,
                                unit_codes_file,
                                db_url,
                                skip_duplicates,
                                use_copy,
                                result_queue,
                                progress_queue,
                                status_queue
                            ),
                            name=f"Worker-{worker_id}"
                        )
                        new_p.start()
                        active_workers[new_p] = worker_id
                        next_file_index += 1
                        time.sleep(0.2)
            
            # Collect any remaining results
            while not result_queue.empty():
                result = result_queue.get()
                results.append(result)
            
            time.sleep(1)
                
    except KeyboardInterrupt:
        print("\n\nInterrupted! Stopping workers...")
        for p, _ in active_workers.items():
            if p.is_alive():
                p.terminate()
                p.join()
        os.unlink(unit_codes_file)
        return
    finally:
        if os.path.exists(unit_codes_file):
            os.unlink(unit_codes_file)
    
    total_duration = (datetime.now() - start_time).total_seconds()
    
    # Summary
    print("\n" + "="*80)
    print(" "*30 + "üìà IMPORT SUMMARY üìà")
    print("="*80)
    
    total_imported = 0
    total_filtered = 0
    total_rows_processed = 0
    
    for result in sorted(results, key=lambda x: x.get('file', '')):
        if 'error' not in result:
            print(f"\nüìÅ {result['file']}")
            print(f"  ‚úÖ Imported: {result.get('records_imported', 0):,}")
            print(f"  üö´ Filtered: {result.get('records_filtered', 0):,} ({result.get('filter_rate', 0):.1f}%)")
            
            total_imported += result.get('records_imported', 0)
            total_filtered += result.get('records_filtered', 0)
            total_rows_processed += result.get('total_rows', 0)
    
    print(f"\nüìä Final Statistics:")
    print(f"  ‚Ä¢ Total files processed: {len(results)}")
    print(f"  ‚Ä¢ Total rows processed: {total_rows_processed:,}")
    print(f"  ‚Ä¢ Total records imported: {total_imported:,}")
    print(f"  ‚Ä¢ Total records filtered: {total_filtered:,}")
    print(f"  ‚Ä¢ Success rate: {total_imported/(total_imported+total_filtered)*100:.1f}%" if (total_imported+total_filtered) > 0 else "0%")
    print(f"\n‚è±Ô∏è  Performance:")
    print(f"  ‚Ä¢ Total time: {total_duration/60:.1f} minutes")
    print(f"  ‚Ä¢ Overall rate: {total_imported/total_duration:.0f} records/second" if total_duration > 0 else "")
    print(f"  ‚Ä¢ Processing speed: {total_rows_processed/total_duration:.0f} rows/second" if total_duration > 0 else "")
    print(f"\nüíæ Database:")
    print(f"  ‚Ä¢ Initial ENTSOE records: {initial_stats['entsoe_count']:,}")
    print(f"  ‚Ä¢ Records added: {total_imported:,}")
    
    print("\n" + "="*80)
    print(" "*20 + "‚ú® ENTSOE IMPORT COMPLETED ‚ú®")
    print("="*80)
    print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Optimized ENTSOE parallel import')
    parser.add_argument('--workers', type=int, default=4, help='Number of workers')
    parser.add_argument('--no-copy', action='store_true', help='Disable COPY optimization')
    parser.add_argument('--skip-duplicates', action='store_true', help='Check for duplicates')
    parser.add_argument('--max-files', type=int, help='Maximum number of files to process (for testing)')
    
    args = parser.parse_args()
    
    # Check system
    print(f"System: {cpu_count()} CPUs, {psutil.virtual_memory().total/1e9:.1f}GB RAM")
    
    if not HAS_OPENPYXL:
        print("\nüí° TIP: Install openpyxl for faster Excel reading:")
        print("   poetry add openpyxl")
    
    import_parallel_optimized(
        num_workers=args.workers,
        skip_duplicates=args.skip_duplicates,
        use_copy=not args.no_copy,
        max_files=args.max_files
    )